{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import math\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Import pre-trained Word Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_model = gensim.models.KeyedVectors.load_word2vec_format('../../Downloads/GoogleNews-vectors-negative300.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "google_model['nasa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the list of words for which the google model has a vector\n",
    "google_vocab = google_model.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Find Word Embeddings\n",
    "\n",
    "### 2.1- Embeddings of Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>articleHeadline</th>\n",
       "      <th>predictedStance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meijer is offering 100 off Back to School coup...</td>\n",
       "      <td>100 Meijer Coupon - Snopes.com</td>\n",
       "      <td>against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meijer is offering 100 off Back to School coup...</td>\n",
       "      <td>Fake Meijer 100 back-to-school coupon goes vir...</td>\n",
       "      <td>against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meijer is offering 100 off Back to School coup...</td>\n",
       "      <td>100 Meijer Coupon - Hoax - Trendolizer</td>\n",
       "      <td>against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meijer is offering 100 off Back to School coup...</td>\n",
       "      <td>100 OFF Meijer Coupon Deals April 2017 HotDeal...</td>\n",
       "      <td>against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meijer is offering 100 off Back to School coup...</td>\n",
       "      <td>10 off 100 in Visa Gift Cards at Meijer - Freq...</td>\n",
       "      <td>against</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  Meijer is offering 100 off Back to School coup...   \n",
       "1  Meijer is offering 100 off Back to School coup...   \n",
       "2  Meijer is offering 100 off Back to School coup...   \n",
       "3  Meijer is offering 100 off Back to School coup...   \n",
       "4  Meijer is offering 100 off Back to School coup...   \n",
       "\n",
       "                                     articleHeadline predictedStance  \n",
       "0                     100 Meijer Coupon - Snopes.com         against  \n",
       "1  Fake Meijer 100 back-to-school coupon goes vir...         against  \n",
       "2             100 Meijer Coupon - Hoax - Trendolizer         against  \n",
       "3  100 OFF Meijer Coupon Deals April 2017 HotDeal...         against  \n",
       "4  10 off 100 in Visa Gift Cards at Meijer - Freq...         against  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_claims = pd.read_csv('Snopes_articles_with_predicted_stance.csv', index_col = 0)\n",
    "my_claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes in a string. Tokenizes it, removes any punctuation in it\n",
    "#Does NOT change the case of the words though.\n",
    "def preprocessing(text):\n",
    "    if pd.isnull(text): #if article doesn't exist. Some claims have <10 articles.\n",
    "        return \"\"\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c,\"\")\n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For a claim or an articleHeadline, this function will return the word embedding for it\n",
    "def calc_vector(current_text, previous_text, previous_vector):\n",
    "    \n",
    "    #if article doesn't exist. Some claims have <10 articles\n",
    "    if pd.isnull(current_text):\n",
    "        return np.zeros(300)\n",
    "    \n",
    "    #as each claim is repeated many times, this will be efficient for the 2nd through 10th time that we are finding the vector \n",
    "    #for a particular claim\n",
    "    if current_text==previous_text:\n",
    "        return previous_vector\n",
    "    \n",
    "    #if this is the first time we are finding the vector for this claim\n",
    "    words = preprocessing(current_text)\n",
    "    \n",
    "    text_vec = []\n",
    "    for word in words:\n",
    "        #checking if the word as written is in google model\n",
    "        if word in google_vocab:\n",
    "            text_vec =+ google_model[word]\n",
    "        \n",
    "        #checking if word in lowercase is in google model\n",
    "        elif word.lower() in google_vocab:\n",
    "            text_vec =+ google_model[word.lower()]\n",
    "        \n",
    "        #checking if word in uppercase is in google model\n",
    "        elif word.upper() in google_vocab:\n",
    "            text_vec =+ google_model[word.upper()]\n",
    "        \n",
    "        #checking if word in capital case (first character capitalized) is in model\n",
    "        elif word.capitalize() in google_vocab:\n",
    "            text_vec =+ google_model[word.capitalize()]\n",
    "        \n",
    "        #if not, just leave text_vec unchanged\n",
    "        else:\n",
    "            text_vec = text_vec\n",
    "    \n",
    "    #updating previous text and previous vector\n",
    "    previous_text = current_text\n",
    "    previous_vector = text_vec\n",
    "    \n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "claim_word_embeddings = [] #This will be the first column in the new dataframe. 'claimWordEmbedding'\n",
    "previous_claim = ''\n",
    "previous_vector =[]\n",
    "for claim in my_claims['claim']:\n",
    "    claim_word_embeddings.append(calc_vector(claim, previous_claim, previous_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46754"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(claim_word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have the word embeddings for the claims. Now I have to get the word embeddings for the articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.2 Word Embeddings of Article Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>articleHeadline</th>\n",
       "      <th>predictedStance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meijer is offering 100 off Back to School coup...</td>\n",
       "      <td>100 Meijer Coupon - Snopes.com</td>\n",
       "      <td>against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meijer is offering 100 off Back to School coup...</td>\n",
       "      <td>Fake Meijer 100 back-to-school coupon goes vir...</td>\n",
       "      <td>against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Meijer is offering 100 off Back to School coup...</td>\n",
       "      <td>100 Meijer Coupon - Hoax - Trendolizer</td>\n",
       "      <td>against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meijer is offering 100 off Back to School coup...</td>\n",
       "      <td>100 OFF Meijer Coupon Deals April 2017 HotDeal...</td>\n",
       "      <td>against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meijer is offering 100 off Back to School coup...</td>\n",
       "      <td>10 off 100 in Visa Gift Cards at Meijer - Freq...</td>\n",
       "      <td>against</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  Meijer is offering 100 off Back to School coup...   \n",
       "1  Meijer is offering 100 off Back to School coup...   \n",
       "2  Meijer is offering 100 off Back to School coup...   \n",
       "3  Meijer is offering 100 off Back to School coup...   \n",
       "4  Meijer is offering 100 off Back to School coup...   \n",
       "\n",
       "                                     articleHeadline predictedStance  \n",
       "0                     100 Meijer Coupon - Snopes.com         against  \n",
       "1  Fake Meijer 100 back-to-school coupon goes vir...         against  \n",
       "2             100 Meijer Coupon - Hoax - Trendolizer         against  \n",
       "3  100 OFF Meijer Coupon Deals April 2017 HotDeal...         against  \n",
       "4  10 off 100 in Visa Gift Cards at Meijer - Freq...         against  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Snopes_articles_with_predicted_stance.csv', index_col = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#claim_embeddings is a list of numpy arrays. This should be a list of lists of numpy arrays\n",
    "article_embeddings = []\n",
    "articles = df['articleHeadline']\n",
    "multiples = math.floor(df.shape[0]/10)\n",
    "\n",
    "previous_claim2 = \"\"\n",
    "previous_vector2 = []\n",
    "\n",
    "for i in range(10):\n",
    "    article_embeddings.append([])\n",
    "    \n",
    "for i in range(df.shape[0]):\n",
    "    current = calc_vector(articles[i], previous_claim2, previous_vector2)\n",
    "    article_embeddings[i%10].append(current)\n",
    "#         article_embeddings[i].append(calc_vector(articles[i + num*10], previous_claim2, previous_vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4676\n",
      "4676\n",
      "4676\n",
      "4676\n",
      "4675\n",
      "4675\n",
      "4675\n",
      "4675\n",
      "4675\n",
      "4675\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(article_embeddings[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, article_embeddings is a list. Each element of article_embeddings[5], for example, is the fifth article for every claim. Currently, because the number of articles is not a multiple of 10, the lists are not of equal size. So I will add dummy vectors to make them even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4676\n",
      "4676\n",
      "4676\n",
      "4676\n",
      "4676\n",
      "4676\n",
      "4676\n",
      "4676\n",
      "4676\n",
      "4676\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 10):\n",
    "    article_embeddings[i].append(np.zeros(300))\n",
    "    \n",
    "for i in range(10):\n",
    "    print(len(article_embeddings[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now they are even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I have all the word embeddings I need. I will try building an XGBoost model just with the word embeddings and see if it is respectable. Otherwise, I will create the following binary features for every article - \n",
    "\n",
    "    - whether it contains a hedging word\n",
    "    - whether it contains a question mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
