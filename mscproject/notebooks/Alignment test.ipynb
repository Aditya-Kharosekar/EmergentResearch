{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/williamferreira/PycharmProjects/mscproject/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model.utils import get_aligned_data, get_dataset, get_tokenized_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aligned_data = get_aligned_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match(w):\n",
    "    return (w in ['some', 'all', 'not', 'every', 'each'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The president of Argentina became the adopted godmother of a boy to help stop him from becoming a werewolf\n",
      "Argentina's President Adopts Young Boy so He will not Turn Into Werewolf\n",
      "for\n",
      "Teacher had sex with 16-year-old\n",
      "Lucita Sandoval sex tape hoax: Viral Argentine video features college student not 16-year-old boy\n",
      "against\n",
      "Guns N Roses signer Axel Rose is dead\n",
      "Axl Rose NOT Dead: Fake MSNBC Death Hoax Goes Viral On Facebook\n",
      "against\n",
      "Anonymous street artist Banksy was arrested\n",
      "Everybody Relax, Banksy was not Arrested And Exposed As A 35-Year-Old Man\n",
      "against\n",
      "The Batmobile was stolen\n",
      "BATMOBILE NOT STOLEN, MTV CONTEST PRIZE REMAINS UNCLAIMED\n",
      "against\n",
      "The Batmobile was stolen\n",
      "Batmobile was not stolen: Cops\n",
      "against\n",
      "A bird pooped on Vladimir Putin during a speech\n",
      "No, a bird did not poop on Vladimir Putin\n",
      "against\n",
      "NBC meteorologist Mike Seidel was caught relieving himself on camera\n",
      "Weather Channel's Mike Seidel was not caught with his pants down\n",
      "against\n",
      "Elon University has banned use of the term freshman\n",
      "Elon University has not banned the term freshman, despite rumors\n",
      "against\n",
      "Fort Carson is locked down because of a missing nuclear weapon\n",
      "Missing Fort Carson item not a nuclear weapon - despite Internet rumors\n",
      "against\n",
      "Canadian-Israeli Gill Rosenberg captured by ISIS while fighting with Kurdish forces\n",
      "YPG Confirms: Gill Rosenberg Not Captured in Kobani\n",
      "against\n",
      "Canadian-Israeli Gill Rosenberg captured by ISIS while fighting with Kurdish forces\n",
      "YPG Confirms: Gill Rosenberg Not Captured in Kobani\n",
      "against\n",
      "YouTube star Josh Paler Lin gave a homeless man 100 and filmed how he spent it without the man's knowledge\n",
      "This Selfless Homeless Man Receives 100 does not Spend A Dime On Himself\n",
      "for\n",
      "Apple added additional reinforcement to its iPhone 6 Plus to ensure it does not bend\n",
      "Debunk: No, Apple did not change up iPhone 6 Plus internals to avoid bending\n",
      "against\n",
      "Some ISIS fighters have contracted Ebola\n",
      "Iraqi media says ISIS militants have contracted Ebola\n",
      "observing\n",
      "Some ISIS fighters have contracted Ebola\n",
      "ISIS cracks down on five confirmed Ebola cases among fighters: official\n",
      "observing\n",
      "Some ISIS fighters have contracted Ebola\n",
      "IRAQI AND KURDISH MEDIA REPORTS: ISIS FIGHTERS HAVE CONTRACTED EBOLA\n",
      "observing\n",
      "Some ISIS fighters have contracted Ebola\n",
      "Officials Refute Iraqi Media Reports That ISIS Members Have Contracted Ebola In Mosul\n",
      "against\n",
      "Some ISIS fighters have contracted Ebola\n",
      "ISIS Militants In Mosul Have Contracted Ebola, Iraqi Media Sources Claim\n",
      "observing\n",
      "Some ISIS fighters have contracted Ebola\n",
      "ISIS Militants Allegedly Contracted Ebola\n",
      "observing\n",
      "Some ISIS fighters have contracted Ebola\n",
      "ISIS Getting Ebola\n",
      "for\n",
      "Some ISIS fighters have contracted Ebola\n",
      "Iraqi media: ISIS militants have contracted Ebola\n",
      "observing\n",
      "ISIS is harvesting and selling human organs to help fund its operations\n",
      "UN urged to investigate ISIS's bloody trade in human organs after Iraqi ambassador reveals doctors are being executed for not harvesting body parts\n",
      "for\n",
      "ISIS leader Abu Bakr al-Baghdadi was killed by an U.S. airstrike\n",
      "Islamic State Leader al-Baghdadi Not Dead\n",
      "against\n",
      "Judd Nelson is dead\n",
      "Judd Nelson is not dead report is a hoax\n",
      "against\n",
      "Robert Plant ripped up an 800 million contract offer to reunite Led Zeppelin\n",
      "No, Robert Plant did not Rip Up an 800 Million Contract\n",
      "against\n",
      "Macaulay Culkin is dead\n",
      "Macaulay Culkin has not Died Despite What Everyone Is Saying On Facebook\n",
      "against\n",
      "Macaulay Culkin is dead\n",
      "Macaulay Culkin NOT Dead: Facebook, Twitter Death Hoax Goes Viral\n",
      "against\n",
      "Pope Francis told a boy whose dog had died that Paradise is open to all of God's creatures\n",
      "NEWS All Dogs Go to Heaven Pope Francis Confirms Paradise Is Open to All of God's Creatures\n",
      "for\n",
      "Pope Francis told a boy whose dog had died that Paradise is open to all of God's creatures\n",
      "Do all dogs go to heaven? Pope tells grieving boy: Paradise is open to all of God's creatures\n",
      "for\n"
     ]
    }
   ],
   "source": [
    "for i, (_, s) in enumerate(dataset.iterrows()):\n",
    "    idx = aligned_data.get((s.claimId, s.articleId))\n",
    "    if idx:\n",
    "        claim_tok = get_tokenized_lemmas(s.claimHeadline)\n",
    "        article_tok = get_tokenized_lemmas(s.articleHeadline)\n",
    "        for x, y in idx:\n",
    "            if x > 0 and y == 0:\n",
    "                f = match(claim_tok[x-1])\n",
    "            elif x == 0 and y > 0:\n",
    "                f = match(article_tok[y-1])\n",
    "            elif [x-1, y-1] not in idx:\n",
    "                f = match(claim_tok[x-1]) or match(article_tok[y-1])\n",
    "        if f:\n",
    "            print s.claimHeadline\n",
    "            print s.articleHeadline\n",
    "            print s.articleHeadlineStance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from jsonrpc import ServerProxy, JsonRpc20, TransportTcpIp\n",
    "import jsonrpclib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from config import *\n",
    "\n",
    "\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__(self):\n",
    "        self.server = ServerProxy(JsonRpc20(),\n",
    "                                  TransportTcpIp(addr=(\"127.0.0.1\", 8080)))\n",
    "    \n",
    "    def parse(self, text):\n",
    "        return json.loads(self.server.parse(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = StanfordNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'sentences': [{u'dependencies': [[u'root', u'ROOT-0', u'Russian-1'],\n",
       "    [u'nn', u'Hoax-5', u'Yeti-2'],\n",
       "    [u'conj_or', u'Yeti-2', u'Bigfoot-4'],\n",
       "    [u'dep', u'Russian-1', u'Hoax-5']],\n",
       "   u'parsetree': u'(ROOT (NP (NP (JJ Russian)) (NP (NNP Yeti) (CC Or) (NNP Bigfoot) (NNP Hoax)) (. ?)))',\n",
       "   u'text': u'Russian Yeti Or Bigfoot Hoax?',\n",
       "   u'words': [[u'Russian',\n",
       "     {u'CharacterOffsetBegin': u'0',\n",
       "      u'CharacterOffsetEnd': u'7',\n",
       "      u'Lemma': u'russian',\n",
       "      u'NamedEntityTag': u'MISC',\n",
       "      u'PartOfSpeech': u'JJ'}],\n",
       "    [u'Yeti',\n",
       "     {u'CharacterOffsetBegin': u'8',\n",
       "      u'CharacterOffsetEnd': u'12',\n",
       "      u'Lemma': u'Yeti',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNP'}],\n",
       "    [u'Or',\n",
       "     {u'CharacterOffsetBegin': u'13',\n",
       "      u'CharacterOffsetEnd': u'15',\n",
       "      u'Lemma': u'or',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'CC'}],\n",
       "    [u'Bigfoot',\n",
       "     {u'CharacterOffsetBegin': u'16',\n",
       "      u'CharacterOffsetEnd': u'23',\n",
       "      u'Lemma': u'Bigfoot',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNP'}],\n",
       "    [u'Hoax',\n",
       "     {u'CharacterOffsetBegin': u'24',\n",
       "      u'CharacterOffsetEnd': u'28',\n",
       "      u'Lemma': u'Hoax',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNP'}],\n",
       "    [u'?',\n",
       "     {u'CharacterOffsetBegin': u'28',\n",
       "      u'CharacterOffsetEnd': u'29',\n",
       "      u'Lemma': u'?',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'.'}]]}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.parse(\"Russian Yeti Or Bigfoot Hoax?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model.utils import get_tokenized_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f1c86b97f57e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_tokenized_lemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/williamferreira/PycharmProjects/mscproject/src/model/utils.pyc\u001b[0m in \u001b[0;36mget_tokenized_lemmas\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_tokenized_lemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnormalize_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/williamferreira/anaconda/envs/mscproject27/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    102\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/williamferreira/anaconda/envs/mscproject27/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/williamferreira/anaconda/envs/mscproject27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \"\"\"\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/williamferreira/anaconda/envs/mscproject27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \"\"\"\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/williamferreira/anaconda/envs/mscproject27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/williamferreira/anaconda/envs/mscproject27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \"\"\"\n\u001b[1;32m   1303\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/williamferreira/anaconda/envs/mscproject27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/williamferreira/anaconda/envs/mscproject27/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "get_tokenized_lemmas(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = [(1, 2), (3, 4), (5, 6), (7, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "align1, align2 = zip(*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'sentences': [{u'dependencies': [[u'root', u'ROOT-0', u'Say-4'],\n",
       "    [u'num', u'Congressmen-3', u'Two-1'],\n",
       "    [u'nn', u'Congressmen-3', u'GOP-2'],\n",
       "    [u'nsubj', u'Say-4', u'Congressmen-3'],\n",
       "    [u'amod', u'Terrorists-6', u'Suspected-5'],\n",
       "    [u'iobj', u'Say-4', u'Terrorists-6'],\n",
       "    [u'nn', u'Border-10', u'Caught-7'],\n",
       "    [u'nn', u'Border-10', u'Crossing-8'],\n",
       "    [u'nn', u'Border-10', u'U.S.Mexico-9'],\n",
       "    [u'dobj', u'Say-4', u'Border-10']],\n",
       "   u'parsetree': u'(ROOT (NP (S (NP (CD Two) (NNP GOP) (NNPS Congressmen)) (VP (VB Say) (NP (JJ Suspected) (NNPS Terrorists)) (NP (NNP Caught) (NNP Crossing) (NNP U.S.Mexico) (NNP Border))))))',\n",
       "   u'text': u'Two GOP Congressmen Say Suspected Terrorists Caught Crossing U.S.Mexico Border',\n",
       "   u'words': [[u'Two',\n",
       "     {u'CharacterOffsetBegin': u'0',\n",
       "      u'CharacterOffsetEnd': u'3',\n",
       "      u'Lemma': u'two',\n",
       "      u'NamedEntityTag': u'NUMBER',\n",
       "      u'NormalizedNamedEntityTag': u'2.0',\n",
       "      u'PartOfSpeech': u'CD'}],\n",
       "    [u'GOP',\n",
       "     {u'CharacterOffsetBegin': u'4',\n",
       "      u'CharacterOffsetEnd': u'7',\n",
       "      u'Lemma': u'GOP',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNP'}],\n",
       "    [u'Congressmen',\n",
       "     {u'CharacterOffsetBegin': u'8',\n",
       "      u'CharacterOffsetEnd': u'19',\n",
       "      u'Lemma': u'Congressmen',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNPS'}],\n",
       "    [u'Say',\n",
       "     {u'CharacterOffsetBegin': u'20',\n",
       "      u'CharacterOffsetEnd': u'23',\n",
       "      u'Lemma': u'say',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'VB'}],\n",
       "    [u'Suspected',\n",
       "     {u'CharacterOffsetBegin': u'24',\n",
       "      u'CharacterOffsetEnd': u'33',\n",
       "      u'Lemma': u'suspected',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'JJ'}],\n",
       "    [u'Terrorists',\n",
       "     {u'CharacterOffsetBegin': u'34',\n",
       "      u'CharacterOffsetEnd': u'44',\n",
       "      u'Lemma': u'Terrorists',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNPS'}],\n",
       "    [u'Caught',\n",
       "     {u'CharacterOffsetBegin': u'45',\n",
       "      u'CharacterOffsetEnd': u'51',\n",
       "      u'Lemma': u'Caught',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNP'}],\n",
       "    [u'Crossing',\n",
       "     {u'CharacterOffsetBegin': u'52',\n",
       "      u'CharacterOffsetEnd': u'60',\n",
       "      u'Lemma': u'Crossing',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNP'}],\n",
       "    [u'U.S.Mexico',\n",
       "     {u'CharacterOffsetBegin': u'61',\n",
       "      u'CharacterOffsetEnd': u'71',\n",
       "      u'Lemma': u'U.S.Mexico',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNP'}],\n",
       "    [u'Border',\n",
       "     {u'CharacterOffsetBegin': u'72',\n",
       "      u'CharacterOffsetEnd': u'78',\n",
       "      u'Lemma': u'Border',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNP'}]]}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.parse(\"Two GOP Congressmen Say Suspected Terrorists Caught Crossing U.S.Mexico Border\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/williamferreira/PycharmProjects/mscproject/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from model.utils import get_w2vec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a23eb3a2e143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0madd_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmult_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_w2vec_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "(add_data, mult_data) = get_w2vec_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'coref': [[[[u'May', 0, 3, 3, 4], [u'May', 0, 1, 1, 2]]]],\n",
       " u'sentences': [{u'dependencies': [[u'root', u'ROOT-0', u'Not-5'],\n",
       "    [u'nn', u'May-2', u'Apple-1'],\n",
       "    [u'nsubj', u'Not-5', u'May-2'],\n",
       "    [u'conj_or', u'May-2', u'May-4'],\n",
       "    [u'ccomp', u'Not-5', u'Hold-6'],\n",
       "    [u'det', u'Event-8', u'An-7'],\n",
       "    [u'dep', u'Hold-6', u'Event-8'],\n",
       "    [u'dep', u'Event-8', u'On-9'],\n",
       "    [u'det', u'24th-11', u'The-10'],\n",
       "    [u'pobj', u'On-9', u'24th-11'],\n",
       "    [u'nn', u'Rumor-14', u'February-13'],\n",
       "    [u'prep_of', u'24th-11', u'Rumor-14']],\n",
       "   u'parsetree': u'(ROOT (S (NP (NNP Apple) (NNP May) (CC Or) (NNP May)) (VP (RB Not) (S (VP (VB Hold) (S (NP (DT An) (NN Event)) (PP (IN On) (NP (NP (DT The) (JJ 24th)) (PP (IN Of) (NP (NNP February) (NNP Rumor)))))))))))',\n",
       "   u'text': u'Apple May Or May Not Hold An Event On The 24th Of February Rumor',\n",
       "   u'words': [[u'Apple',\n",
       "     {u'CharacterOffsetBegin': u'0',\n",
       "      u'CharacterOffsetEnd': u'5',\n",
       "      u'Lemma': u'Apple',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNP'}],\n",
       "    [u'May',\n",
       "     {u'CharacterOffsetBegin': u'6',\n",
       "      u'CharacterOffsetEnd': u'9',\n",
       "      u'Lemma': u'May',\n",
       "      u'NamedEntityTag': u'DATE',\n",
       "      u'NormalizedNamedEntityTag': u'XXXX-05',\n",
       "      u'PartOfSpeech': u'NNP',\n",
       "      u'Timex': u'<TIMEX3 tid=\"t1\" type=\"DATE\" value=\"XXXX-05\">May</TIMEX3>'}],\n",
       "    [u'Or',\n",
       "     {u'CharacterOffsetBegin': u'10',\n",
       "      u'CharacterOffsetEnd': u'12',\n",
       "      u'Lemma': u'or',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'CC'}],\n",
       "    [u'May',\n",
       "     {u'CharacterOffsetBegin': u'13',\n",
       "      u'CharacterOffsetEnd': u'16',\n",
       "      u'Lemma': u'May',\n",
       "      u'NamedEntityTag': u'DATE',\n",
       "      u'NormalizedNamedEntityTag': u'XXXX-05',\n",
       "      u'PartOfSpeech': u'NNP',\n",
       "      u'Timex': u'<TIMEX3 tid=\"t2\" type=\"DATE\" value=\"XXXX-05\">May</TIMEX3>'}],\n",
       "    [u'Not',\n",
       "     {u'CharacterOffsetBegin': u'17',\n",
       "      u'CharacterOffsetEnd': u'20',\n",
       "      u'Lemma': u'not',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'RB'}],\n",
       "    [u'Hold',\n",
       "     {u'CharacterOffsetBegin': u'21',\n",
       "      u'CharacterOffsetEnd': u'25',\n",
       "      u'Lemma': u'hold',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'VB'}],\n",
       "    [u'An',\n",
       "     {u'CharacterOffsetBegin': u'26',\n",
       "      u'CharacterOffsetEnd': u'28',\n",
       "      u'Lemma': u'a',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'DT'}],\n",
       "    [u'Event',\n",
       "     {u'CharacterOffsetBegin': u'29',\n",
       "      u'CharacterOffsetEnd': u'34',\n",
       "      u'Lemma': u'event',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NN'}],\n",
       "    [u'On',\n",
       "     {u'CharacterOffsetBegin': u'35',\n",
       "      u'CharacterOffsetEnd': u'37',\n",
       "      u'Lemma': u'on',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'IN'}],\n",
       "    [u'The',\n",
       "     {u'CharacterOffsetBegin': u'38',\n",
       "      u'CharacterOffsetEnd': u'41',\n",
       "      u'Lemma': u'the',\n",
       "      u'NamedEntityTag': u'DATE',\n",
       "      u'NormalizedNamedEntityTag': u'XXXX-02-24',\n",
       "      u'PartOfSpeech': u'DT',\n",
       "      u'Timex': u'<TIMEX3 tid=\"t3\" type=\"DATE\" value=\"XXXX-02-24\">The 24th Of February</TIMEX3>'}],\n",
       "    [u'24th',\n",
       "     {u'CharacterOffsetBegin': u'42',\n",
       "      u'CharacterOffsetEnd': u'46',\n",
       "      u'Lemma': u'24th',\n",
       "      u'NamedEntityTag': u'DATE',\n",
       "      u'NormalizedNamedEntityTag': u'XXXX-02-24',\n",
       "      u'PartOfSpeech': u'JJ',\n",
       "      u'Timex': u'<TIMEX3 tid=\"t3\" type=\"DATE\" value=\"XXXX-02-24\">The 24th Of February</TIMEX3>'}],\n",
       "    [u'Of',\n",
       "     {u'CharacterOffsetBegin': u'47',\n",
       "      u'CharacterOffsetEnd': u'49',\n",
       "      u'Lemma': u'of',\n",
       "      u'NamedEntityTag': u'DATE',\n",
       "      u'NormalizedNamedEntityTag': u'XXXX-02-24',\n",
       "      u'PartOfSpeech': u'IN',\n",
       "      u'Timex': u'<TIMEX3 tid=\"t3\" type=\"DATE\" value=\"XXXX-02-24\">The 24th Of February</TIMEX3>'}],\n",
       "    [u'February',\n",
       "     {u'CharacterOffsetBegin': u'50',\n",
       "      u'CharacterOffsetEnd': u'58',\n",
       "      u'Lemma': u'February',\n",
       "      u'NamedEntityTag': u'DATE',\n",
       "      u'NormalizedNamedEntityTag': u'XXXX-02-24',\n",
       "      u'PartOfSpeech': u'NNP',\n",
       "      u'Timex': u'<TIMEX3 tid=\"t3\" type=\"DATE\" value=\"XXXX-02-24\">The 24th Of February</TIMEX3>'}],\n",
       "    [u'Rumor',\n",
       "     {u'CharacterOffsetBegin': u'59',\n",
       "      u'CharacterOffsetEnd': u'64',\n",
       "      u'Lemma': u'Rumor',\n",
       "      u'NamedEntityTag': u'O',\n",
       "      u'PartOfSpeech': u'NNP'}]]}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.parse(\"Apple May Or May Not Hold An Event On The 24th Of February Rumor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
